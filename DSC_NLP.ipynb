{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DSC NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcOLHkUvLBvr",
        "colab_type": "code",
        "outputId": "5a0eb817-d789-4acc-e75e-90caca438e50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from   nltk.stem     import PorterStemmer\n",
        "from   nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from   nltk.corpus   import stopwords\n",
        "from   nltk.tokenize import RegexpTokenizer\n",
        "from   collections   import Counter\n",
        "import nltk\n",
        " \n",
        "nltk.download('punkt')\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "import glob\n",
        "import errno\n",
        "import os \n",
        "import re # Regular Expressions \n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import pandas as pd  \n",
        "import numpy as np "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5PlTEVzMl57",
        "colab_type": "text"
      },
      "source": [
        "**NLTK**\n",
        "Natural Language Tool Kit (NLTK) is a Python library to make programs that work with natural language. It provides a user-friendly interface to datasets that are over 50 corpora and lexical resources such as WordNet Word repository. The library can perform different operations such as tokenizing, stemming, classification, parsing, tagging, and semantic reasoning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxBEOEos49ro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-Z9EKqg5IVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords.words('arabic')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpOokhBb5yjQ",
        "colab_type": "code",
        "outputId": "6940ed61-abf1-4a71-c7c2-370d89feb81f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "documentA = 'the man went out for a walk'\n",
        "documentB = 'the children sat around the fire'\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform([documentA, documentB])\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "dense = vectors.todense()\n",
        "denselist = dense.tolist()\n",
        "df = pd.DataFrame(denselist, columns=feature_names)\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>around</th>\n",
              "      <th>children</th>\n",
              "      <th>fire</th>\n",
              "      <th>for</th>\n",
              "      <th>man</th>\n",
              "      <th>out</th>\n",
              "      <th>sat</th>\n",
              "      <th>the</th>\n",
              "      <th>walk</th>\n",
              "      <th>went</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.42616</td>\n",
              "      <td>0.42616</td>\n",
              "      <td>0.42616</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.303216</td>\n",
              "      <td>0.42616</td>\n",
              "      <td>0.42616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.407401</td>\n",
              "      <td>0.407401</td>\n",
              "      <td>0.407401</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.407401</td>\n",
              "      <td>0.579739</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     around  children      fire      for  ...       sat       the     walk     went\n",
              "0  0.000000  0.000000  0.000000  0.42616  ...  0.000000  0.303216  0.42616  0.42616\n",
              "1  0.407401  0.407401  0.407401  0.00000  ...  0.407401  0.579739  0.00000  0.00000\n",
              "\n",
              "[2 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar3BsGkm7zHX",
        "colab_type": "code",
        "outputId": "def36f4e-7172-4dd3-f511-f8b3505fd1cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "vectorizer1 = CountVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer1.fit([documentA, documentB])\n",
        "vector = vectorizer1.transform([documentA, documentB])\n",
        "\n",
        "#feature_names = vector.get_feature_names()\n",
        "dense = vector.todense()\n",
        "denselist = dense.tolist()\n",
        "df = pd.DataFrame(denselist)\n",
        "df\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0  1  2  3  4  5  6  7  8  9\n",
              "0  0  0  0  1  1  1  0  1  1  1\n",
              "1  1  1  1  0  0  0  1  2  0  0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zERuFMNjLC7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#===================================\n",
        "#  Function Counting : This fucntion\n",
        "# count the number of words in the \n",
        "#file and show them in Desc order.\n",
        "# param : Path of the corpus   \n",
        "#                               \n",
        "#==================================\n",
        "def Counting(path):\n",
        "    corpus = glob.glob(path)\n",
        "    for name in corpus:\n",
        "        \n",
        "            with open(name) as f:\n",
        "\n",
        "            #create a list of all words fetched from the file using a list comprehension\n",
        "                words = [word for line in f for word in line.split()]\n",
        "            print (\"### The total word count in file #####\",name,\" is:\", len(words))\n",
        "            \n",
        "            c = Counter(words)\n",
        "            for word, count in c.most_common():\n",
        "               print (word, count)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmwYQP9ILPOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Counting('/content/corpus/*.txt')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tFFgqqKMEYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#=====================================================\n",
        "#Function : word_Search : This function search for a word ,typed by the user ,\n",
        "#in the corpus it test the existence of the word \n",
        "#in each file and each line of the file , and total count #\n",
        "#param : path of the corpus .  #\n",
        "#===============================#\n",
        "\n",
        "def Word_Search(path):\n",
        "    # update the path here , from root to your folder that contains  the text files \n",
        "    #path = '/Users/macbookair/Desktop/corpus/*.txt'\n",
        "    \n",
        "    corpus = glob.glob(path)\n",
        "    string1  = input(\"Please enter the  word you are looking for  : \")\n",
        "    for name in corpus:\n",
        "        # for each file in the corpus \n",
        "        try:\n",
        "            with open(name) as f: # open the file \n",
        "                print('You are looking in File  :', name)\n",
        "                count = 0\n",
        "                nblines=0\n",
        "                for line in f:\n",
        "                    nblines +=1 \n",
        "                    if string1 in line:\n",
        "                        count+=1\n",
        "                        print('----found----------')\n",
        "                    else:\n",
        "                        print('-----The word doesnt exist in this line------- ')\n",
        "                print(\"The word * \",string1,\"* exist in this file :\",count ,\"times\")\n",
        "                print(\"Number of lines of this file   **********\",nblines)   \n",
        "                \n",
        "        except IOError as exc: # if files doesn't exist , throw exeception \n",
        "            if exc.errno != errno.EISDIR:\n",
        "                print('Files not found ')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc2nrSYSM185",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# call the function             \n",
        "Word_Search('/content/corpus/*.txt') \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8Ar9FehNI92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#========================================\n",
        "#Function: StopWord_elem : this function count the number of words in the text file \n",
        "#of the corpus and return the count .\n",
        "#The it applies stop word elimination on them and re calculate the count . \n",
        "#\n",
        "# \n",
        "# param : corpus path  \n",
        "#                             \n",
        "#========================================\n",
        "def StopWordElem(path):\n",
        " \n",
        "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "    from nltk.corpus import stopwords\n",
        "    \n",
        "     \n",
        "    corpus = glob.glob(path)\n",
        "    \n",
        "    for name in corpus:\n",
        "\n",
        "        data=open(name).read().replace('\\n', '') #Sentence tokenize \n",
        "        words = word_tokenize(data) # word tokenizing \n",
        "\n",
        "        stopWords = set(stopwords.words('english')) \n",
        "        wordsFiltered = []\n",
        "        for w in words:\n",
        "                if w not in stopWords:\n",
        "                    wordsFiltered.append(w)\n",
        "         \n",
        "        print (\"The total word count in file :\",name ,'is : ', len(words))\n",
        "        print(\"Number of words after stop words elimination : \",len(wordsFiltered))\n",
        "        print(wordsFiltered)\n",
        "        \n",
        "        print('========The most Frequent Words ======')\n",
        "        words = [word for line in wordsFiltered for word in line.split()]\n",
        "        print (\"The total word count is:\", len(words))\n",
        "                \n",
        "        c = Counter(words)\n",
        "        for word, count in c.most_common():\n",
        "            print (word, count)\n",
        "            \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RY4aEmZOCTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "StopWordElem('/content/corpus/*.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fXgz3vgQncq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "count_vect=CountVectorizer()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2gClEsk_5-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a=count_vect.fit_transform(X)\n",
        "a.toarray()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBMfqo9TAUxq",
        "colab_type": "code",
        "outputId": "0d3da9e7-d232-444c-e46f-b2b097e457dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "def AllVoab(path):\n",
        "    corpus = glob.glob(path)\n",
        "    \n",
        "    dataAll=[]\n",
        "    for name in corpus:\n",
        "       \n",
        "        dataAll.append(open(name).read().replace('\\n', ''))\n",
        "       \n",
        "    for i in dataAll:\n",
        "        \n",
        "        new_data=re.sub(r'\\w*\\d\\w*', '', i)\n",
        "        #remove digits\n",
        "        words = word_tokenize(new_data) \n",
        "        tokenizer = RegexpTokenizer(r'\\w+')\n",
        "        words=tokenizer.tokenize(new_data)\n",
        "\n",
        "\n",
        "        stopWords = set(stopwords.words('english'))\n",
        "       \n",
        "        \n",
        "        for word in list(words):  # iterating on a copy since removing will mess things up\n",
        "            if word in other:\n",
        "                words.remove(word)\n",
        "                \n",
        "                wordsFiltered = []\n",
        "        \n",
        "            for w in words:\n",
        "                    if w not in stopWords:\n",
        "                         \n",
        "                        wordsFiltered.append(w)\n",
        "                        \n",
        "                        \n",
        "                        AllVocStemed=[]\n",
        "lmtzr = WordNetLemmatizer()\n",
        "AllVocStemed = [lmtzr.lemmatize(word) for word in All]\n",
        "AllVocStemed  = list(OrderedDict.fromkeys(AllVocStemed))\n",
        "\n",
        "\n",
        "### Store the vocabulary in a text file . \n",
        "with open('vocabulaire.txt', 'w') as f:\n",
        "    for word in AllVocStemed:\n",
        "        f.write(\"%s\\n\" % word)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-ebd02f032cc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                         \u001b[0mAllVocStemed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mlmtzr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mAllVocStemed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlmtzr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mAllVocStemed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mAllVocStemed\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAllVocStemed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AllVocStemed' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9LPsXT9Agqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import numpy as np \n",
        "from random import randint\n",
        "import pandas as pd\n",
        "import string\n",
        "import chardet\n",
        "with open('data_for_spam.csv', 'rb') as f:\n",
        "    result = chardet.detect(f.read())  # or readline if the file is large\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS3Tvd5sBeOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YFCBXCBESlk",
        "colab_type": "code",
        "outputId": "fb068202-3e4a-48d1-f304-7f859f511c5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "dataset.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>WINNER!! As a valued network customer you have...</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>okmail: Dear Dave this is your final notice to...</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>07732584351 - Rodger Burns - MSG = We tried to...</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>URGENT! This is the 2nd attempt to contact U!U...</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Congrats! Nokia 3650 video camera phone is you...</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>URGENT This is our 2nd attempt to contact U. Y...</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Want explicit SEX in 30 secs? Ring 02073162414...</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Sorry I missed your call let's talk when you h...</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>HEY HEY WERETHE MONKEESPEOPLE SAY WE MONKEYARO...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>CHA QUITEAMUZING THATÕSCOOL BABE,PROBPOP IN &amp; ...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                data target\n",
              "0  WINNER!! As a valued network customer you have...   spam\n",
              "1  okmail: Dear Dave this is your final notice to...   spam\n",
              "2  07732584351 - Rodger Burns - MSG = We tried to...   spam\n",
              "3  URGENT! This is the 2nd attempt to contact U!U...   spam\n",
              "4  Congrats! Nokia 3650 video camera phone is you...   spam\n",
              "5  URGENT This is our 2nd attempt to contact U. Y...   spam\n",
              "6  Want explicit SEX in 30 secs? Ring 02073162414...   spam\n",
              "7  Sorry I missed your call let's talk when you h...   spam\n",
              "8  HEY HEY WERETHE MONKEESPEOPLE SAY WE MONKEYARO...    ham\n",
              "9  CHA QUITEAMUZING THATÕSCOOL BABE,PROBPOP IN & ...    ham"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfc2ZjYJD4hA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset=pd.read_csv('data_for_spam.csv', encoding=result['encoding'])\n",
        "x=dataset.iloc[:,0]\n",
        "y=dataset.iloc[:,1]\n",
        "x=x.to_dict()\n",
        "\n",
        "X=[]\n",
        "for d in range(len(x)):\n",
        "    b=x[d].lower()\n",
        "    sentence= re.sub(r'\\d+','', b)\n",
        "    sentence= re.sub('['+string.punctuation+']', '', sentence)\n",
        "    print(sentence)\n",
        "    X.append(sentence)\n",
        "   \n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx_TSy6tFmYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WemCJGvJFAdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCJyAmsdEQ5Z",
        "colab_type": "code",
        "outputId": "7d028c11-dfcb-4887-d0c4-bf3d79b8a4ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.shape(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLfOHkk3EQf6",
        "colab_type": "code",
        "outputId": "377bd2fe-8aed-4b22-f07e-3f5fe62245f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.shape(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8euwVs6-BFsQ",
        "colab_type": "code",
        "outputId": "2b4137f8-989b-4e21-8bd6-16775e911dfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "dataset=pd.read_csv('data_for_spam.csv', encoding=result['encoding'])\n",
        "x=dataset.iloc[:,0]\n",
        "y=dataset.iloc[:,1]\n",
        "x=x.to_dict()\n",
        "\n",
        "X=[]\n",
        "for d in range(len(x)):\n",
        "    b=x[d].lower()\n",
        "    sentence= re.sub(r'\\d+','', b)\n",
        "    sentence= re.sub('['+string.punctuation+']', '', sentence)\n",
        "    X.append(sentence)\n",
        "    print(X)\n",
        "   \n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "count_vect=CountVectorizer()\n",
        "a=count_vect.fit_transform(X)\n",
        "a.toarray()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n",
        "\n",
        "count_vect=CountVectorizer()\n",
        "X_train_counts=count_vect.fit_transform(X_train)\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "X_train_tfidf.toarray()\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "sm=SMOTE()\n",
        "X_train_res, y_train_res = sm.fit_sample(X_train_tfidf, y_train)\n",
        "\n",
        "#unique, counts = np.unique(y_train_res, return_counts=True)\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "clf= SVC(kernel = 'linear', random_state = 0)\n",
        "clf.fit(X_train_res, y_train_res)\n",
        "clf.score(X_train_res, y_train_res)\n",
        "\n",
        "\n",
        "X_test_tfidf=count_vect.transform(X_test)\n",
        "\n",
        "y_pred=clf.predict(X_test_tfidf)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,recall_score,precision_score\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "Accuracy_Score = accuracy_score(y_test, y_pred)\n",
        "Recall=recall_score(y_test, y_pred, average='weighted')\n",
        "Precision=precision_score(y_test, y_pred, average='weighted')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLtN1PCzCf99",
        "colab_type": "code",
        "outputId": "344cf82b-63fd-40dd-ff90-575a1f5a18d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<113x3 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 7 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rCogHrvBWN4",
        "colab_type": "code",
        "outputId": "6b3b9bb0-1bd5-4b86-e34e-9f714d406ea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(cm)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[6 0]\n",
            " [0 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx_1VUESB8Cp",
        "colab_type": "code",
        "outputId": "b393dae8-22f6-4f01-9d0e-3473e05f4d32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Accuracy_Score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ForgLlLKB9uM",
        "colab_type": "code",
        "outputId": "f3c782cd-a554-4856-d9ca-34c6219affca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "source": [
        "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
        "                      title='Confusion matrix, without normalization')\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
        "                      title='Normalized confusion matrix')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-3a38e1b720ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m plot_confusion_matrix(y_test, y_pred, classes=class_names,\n\u001b[0m\u001b[1;32m      2\u001b[0m                       title='Confusion matrix, without normalization')\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Plot normalized confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plot_confusion_matrix' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRyG_bG9CKpi",
        "colab_type": "code",
        "outputId": "5dd355f8-8b6a-4157-c325-edfc7ae7a9f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# list of text documents\n",
        "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
        "# create the transform\n",
        "vectorizer = CountVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "# summarize\n",
        "print(vectorizer.vocabulary_)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqABrgm7C2HR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wze5wBQUC90s",
        "colab_type": "code",
        "outputId": "4f3bd371-a522-4d23-d43a-c9d974d5dadf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# list of text documents\n",
        "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
        "\t\t\"The dog.\",\n",
        "\t\t\"The fox\"]\n",
        "# create the transform\n",
        "vectorizer = TfidfVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "# summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "print(vectorizer.idf_)\n",
        "# encode document\n",
        "vector = vectorizer.transform([text[0]])\n",
        "# summarize encoded vector\n",
        "print(vector.toarray())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
            "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.        ]\n",
            "[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
            "  0.36388646 0.42983441]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f28b6pPEDIbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}